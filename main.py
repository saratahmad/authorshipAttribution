# -*- coding: utf-8 -*-
"""23100275_23100327_23100003_23100004_23100253_23100176.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VZqtkuvepebrgrStSJ4OKmmXfESrbdKV

**Machine Learning Project: Authorship Attribution**

**Group 2** 
Bismah Najeeb 23100275

Mahnoor Taseer Malik 23100327

Mohammad Jahanzeb Raza 23100003

Muhammad Musa 23100004

Sarat Ahmed 23100253

Syed Talal Hasan 23100176

The following project combines the tweets we extracted in Phase 1 and use KNN and Neural Network models to implement a authorship attribution system.

IMPORT LIBRARIES
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from itertools import chain
import re
from glob import glob
!pip install sentence_transformers
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score,cross_val_predict
from sklearn.metrics import confusion_matrix, classification_report , accuracy_score
from sklearn.model_selection import train_test_split
import statistics
from sklearn.preprocessing import MultiLabelBinarizer
import seaborn as sns
from sklearn.metrics import precision_recall_fscore_support
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_validate 
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier

"""Loading Data:
Read all the CSVS and initialize the embedding model
"""

csvs = []
authors=[]
for i , dfs in enumerate(glob('/content/drive/MyDrive/ML Assignment/Project/ML Phase 1 files/Corrected files/*.csv')):
    temp = dfs.split('/')[-1][:-10]  #extract names of author and save in a list to display in the confusion matrix
    print(temp)
    authors.append(temp)
    df= pd.read_csv(dfs)
    a = np.ones((df.shape[0],1))   # make a unique integer class label for each author starting from 0 going upto i (5)
    to_add = a * i
    to_add = to_add.astype(np.int16)
    df['class'] = to_add  # adding a column of author attribution to each class as a integer representation
    csvs.append(df)
model = SentenceTransformer('all-MiniLM-L6-v2')

'''
Function that returns all the unique words in a csv 
The words are returned as a set
'''

def bow(csv):
    all = []
    for i,row in enumerate(csv.iterrows()):
        try:
            data = row[1][0]
            data=data.split(' ')
            all.append(data)
        except:
            print(i)
            print(row[1])
    all = list(chain.from_iterable(all))    #convert a list of list to a single list
    un=set(all)
    un = list(un)
    return sorted(un)


''''
Function that counts number of instances of each word and compares them with the train data and returns a word vector of the given csv
The return data is a list of list for each rows count of each word
Laplace smoothing is also applied to the data
'''
  
def word_count_vector(csv, train):
  all_words = bow(train)
  # print(all_words)
  # print(len(all_words))
  encode=list()
  for i,row in enumerate(csv.iterrows()):
      word_vector =  [1 for _ in range(len(all_words))]
      data = row[1][0]
      data = str(data)
      data=data.split(' ')
      for d in data:
          try:
              index = all_words.index(d)
              word_vector[index]+=1
          except:
              continue
      encode.append(word_vector)
  return encode

#combinig all the given csvs into a single dataframe
full_data = csvs[0]
for i,csv in enumerate(csvs[1:]):
  full_data = pd.concat([full_data, csv])
full_data = full_data.dropna()
print(full_data.shape)

"""Preparing data for Bag of words predictions"""

data_bow = full_data.values
labels_bow = full_data[["class"]].to_numpy()  #getting the gold labels
data_bow = data_bow[:,0]
train_bow, test_bow , y_train_bow, y_test_bow = train_test_split(data_bow,labels_bow, test_size=0.3, random_state=5, stratify=labels_bow)
train_bow = pd.DataFrame(train_bow)
test_bow = pd.DataFrame(test_bow)
X_train_bow = np.array(word_count_vector(train_bow,train_bow))
X_test_bow =  np.array(word_count_vector(test_bow,train_bow))
#Making sure that the label vector is a 1d vector
y_train_bow = y_train_bow.ravel()
y_test_bow = y_test_bow.ravel()

print(X_train_bow.shape)
print(y_train_bow.shape)

"""Preparing data for embedding predictions"""

s1 = full_data.values
labels = full_data[["class"]].to_numpy()
s1 = s1[:,0]
print(s1.shape)
embeddings = model.encode(s1)

print(embeddings.shape)
print(labels.shape)
print(type(embeddings))
labels = labels.flatten()
print(labels.shape)

#making the data for training and testing at 70:30 split. The data is also stratified for testing

X_train_embedding, X_test_embedding, y_train_embedding, y_test_embedding = train_test_split(embeddings, labels, test_size=0.3, random_state=1 , stratify=labels)
print(X_train_embedding.shape)
print(y_train_embedding.shape)

"""KNN Bag of words"""

#Performing 5 fold cross validation on KNN for each value of k from 1 to 6
e_man_bow=[]
conf_euc_bow=[]
for k in range(1,7):
    KNN = KNeighborsClassifier(n_neighbors=k,p=2)
    cross_val_score(KNN,X_train_bow,y_train_bow,cv=5,scoring='accuracy')
    e_man_bow.append(statistics.mean(cross_val_score(KNN,X_train_bow,y_train_bow,cv=5,scoring='accuracy')))
    conf_euc_bow.append(cross_val_predict(KNN,X_train_bow,y_train_bow,cv=5))

print("Euclidean Distance")
for i in conf_euc_bow:
    conf_mat = confusion_matrix(i,y_train_bow.astype(int))
    hmap_labels = list(range(conf_mat.shape[0]))
    s = sns.heatmap(conf_mat,xticklabels=authors,yticklabels=authors,annot=True)
    s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
    plt.show()
    f1 = classification_report(i,y_train_bow,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']['f1-score']
    print("ACCURACY: ",accuracy_score(i,y_train_bow))
    print("F1 Score: ", f1)
xticks = list(range(1,7))
plt.plot(xticks,e_man_bow,color='blue')
plt.title('Accuracy Comparision')
plt.xticks(xticks)
plt.xlabel('K')
plt.ylabel('Accuracy')

#Using the best value of k perform prediction on the test data, for this instance the highest accuracy was at k = 2

KNN_euclidean = KNeighborsClassifier(n_neighbors=1,p=2)
KNN_euclidean.fit(X_train_bow,y_train_bow)
euclidean_predictions = KNN_euclidean.predict(X_test_bow)

#Displaying the confusion matrix, f1 score, accuracy and macro average on the test datat

print("Euclidean Distance")
conf_mat = confusion_matrix(euclidean_predictions,y_test_bow.astype(int))
hmap_labels = list(range(conf_mat.shape[0]))
s = sns.heatmap(conf_mat,xticklabels=authors,yticklabels=authors,annot=True)
s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
plt.show()
t= classification_report(euclidean_predictions,y_test_bow,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']
precision, recall , f1,= t['precision'] , t['recall'] , t['f1-score'] 
print("ACCURACY: ",accuracy_score(euclidean_predictions,y_test_bow))
print("Precision: ", precision)
print("F1 Score: ", f1)
print("Recall: ", recall)

"""EMBEDDINGS - KNN"""

e_man=[]
conf_euc=[]
# just like for bag of words perform k fold cross val on each value of k from 1-6
for k in range(1,7):
    KNN = KNeighborsClassifier(n_neighbors=k,p=2)
    cross_val_score(KNN,X_train_embedding,y_train_embedding,cv=5,scoring='accuracy')
    e_man.append(statistics.mean(cross_val_score(KNN,X_train_embedding,y_train_embedding,cv=5,scoring='accuracy')))
    conf_euc.append(cross_val_predict(KNN,X_train_embedding,y_train_embedding,cv=5))

'''
Display all the confusion matrix and classification report figures for each value of k
'''

print("Euclidean Distance")
for i in conf_euc:
    conf_mat = confusion_matrix(i,y_train_embedding.astype(int))
    s = sns.heatmap(conf_mat,xticklabels=authors,yticklabels=authors,annot=True)
    s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
    plt.show()
    f1 = classification_report(i,y_train_embedding,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']['f1-score']
    print("ACCURACY: ",accuracy_score(i,y_train_embedding))
    print("F1 Score: ", f1)
xticks = list(range(1,7))
plt.plot(xticks,e_man,color='blue')
plt.title('Accuracy Comparision')
plt.xlabel('K')
plt.ylabel('Accuracy')

# Make predictions on the best value of k which is 4 for this instance

KNN_euclidean = KNeighborsClassifier(n_neighbors=4,p=2)
KNN_euclidean.fit(X_train_embedding,y_train_embedding)
euclidean_predictions = KNN_euclidean.predict(X_test_embedding)

# Report accuracy and F1 score of the test data

print("Euclidean Distance")
conf_mat = confusion_matrix(euclidean_predictions,y_test_embedding.astype(int))
hmap_labels = list(range(conf_mat.shape[0]))
s = sns.heatmap(conf_mat,xticklabels=authors,yticklabels=authors,annot=True)
s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
plt.show()
t= classification_report(euclidean_predictions,y_test_embedding,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']
precision, recall , f1,= t['precision'] , t['recall'] , t['f1-score'] 
print("ACCURACY: ",accuracy_score(euclidean_predictions,y_test_embedding))
print("Precision: ", precision)
print("F1 Score: ", f1)
print("Recall: ", recall)

"""Neural Network - Bag of Words"""

# Define the neural network for Bag of words classification, it has 3 hidden layers with 80 , 64 and 32 neurons respectively, the optimizer is adam
# Iterations to reach minimum are 300 and the batch size for mini batch gradient decent is 100 and a learning rate of 0.0001
# A higher learning rate was giving worst result so reducing the learning rate and increasing the iteration improves our result

clf = MLPClassifier(solver = "adam", batch_size=100,learning_rate_init = 0.0001,  hidden_layer_sizes = (80,64,32,), max_iter=300 , random_state=5)

# Performing 5 fold cross validation
cv_results = cross_validate(clf, X_train_bow, y_train_bow, cv=5, return_train_score=False)

#Display accuracy at each cross fold
xticks = list(range(1,6))
plt.plot(xticks,cv_results['test_score'],color='blue')
plt.title('Accuracy Comparision')
plt.xticks(xticks)
plt.xlabel('K')
plt.ylabel('Accuracy')

clf = MLPClassifier(solver = "adam", batch_size=100,learning_rate_init = 0.0001,  hidden_layer_sizes = (80,64,32,), max_iter=300 , random_state=5)

clf.fit(X_train_bow,y_train_bow)

#Reporting result on the test data of bag of words
pred = clf.predict(X_test_bow)
conf_sk = confusion_matrix(y_test_bow,pred)
s=sns.heatmap(conf_sk, annot=True,xticklabels=authors, yticklabels=authors)
s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
plt.show()
t= classification_report(pred,y_test_bow,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']
precision, recall , f1,= t['precision'] , t['recall'] , t['f1-score'] 
print("ACCURACY: ",accuracy_score(pred,y_test_bow))
print("Precision: ", precision)
print("F1 Score: ", f1)
print("Recall: ", recall)

"""Neural Network - Embeddings




"""

# Define the neural network for Bag of words classification, it has 3 hidden layers with 80 , 64 and 32 neurons respectively, the optimizer is adam
# Iterations to reach minimum are 300 and the batch size for mini batch gradient decent is 100
# using the same parameters for comparitive results between embeddings and bag of words

clf = MLPClassifier(solver = "adam", batch_size=100,learning_rate_init = 0.0001,  hidden_layer_sizes = (80,64,32,), max_iter=300 , random_state=5)

# Performing 5 fold cross validation for embeddings data

cv_results = cross_validate(clf, X_train_embedding, y_train_embedding, cv=5, return_train_score=False)
clf.fit(X_train_embedding,y_train_embedding)

#Reporting result on the test data of embedding

xticks = list(range(1,6))
plt.plot(xticks,cv_results['test_score'],color='blue')
plt.title('Accuracy Comparision')
plt.xticks(xticks)
plt.xlabel('K')
plt.ylabel('Accuracy')

#Making predictions on the test data to display the confusion matrix
pred = clf.predict(X_test_embedding)
conf_sk = confusion_matrix(y_test_embedding,pred)
s=sns.heatmap(conf_sk, annot=True,xticklabels=authors, yticklabels=authors)
s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
plt.show()

#Making predictions on the test data to report accuracy, precision, recall and f1 score

precision, recall , f1,_ = classification_report(pred,y_test_embedding,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']
print("ACCURACY: ",accuracy_score(pred,y_test_embedding))
print("Precision: ", precision)
print("F1 Score: ", f1)
print("Recall: ", recall)

"""Random Forest (Bag of Words)


"""

# Using Random forst as the ensemble method with 50 estimators on bag of words

clf = RandomForestClassifier(n_estimators=50)
clf.fit(X_train_bow, y_train_bow)

#making prediction on test data
pred = clf.predict(X_test_bow)

# Reporting results for test data of bag of words with random forest
conf_sk = confusion_matrix(y_test_bow,pred)
s=sns.heatmap(conf_sk, annot=True,xticklabels=authors, yticklabels=authors)
s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
plt.show()

t = classification_report(pred,y_test_bow,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']
precision, recall , f1,= t['precision'] , t['recall'] , t['f1-score'] 
print("ACCURACY: ",accuracy_score(pred,y_test_bow))
print("Precision: ", precision)
print("F1 Score: ", f1)
print("Recall: ", recall)

"""Random Forest - Embeddings


"""

# Using Random forst as the ensemble method with 50 estimators on embedding

clf = RandomForestClassifier(n_estimators=50)
clf.fit(X_train_embedding, y_train_embedding)

pred = clf.predict(X_test_embedding)

conf_sk = confusion_matrix(y_test_embedding,pred)
s=sns.heatmap(conf_sk, annot=True,xticklabels=authors, yticklabels=authors)
s.set(xlabel="GOLD labels",ylabel="Predicted Labels")
plt.show()

# Reporting results for test data of embedding with random forest

t = classification_report(pred,y_test_embedding,labels=[0,1,2,3,4,5],output_dict=True,zero_division=1)['macro avg']
precision, recall , f1,= t['precision'] , t['recall'] , t['f1-score'] 
print("ACCURACY: ",accuracy_score(pred,y_test_embedding))
print("Precision: ", precision)
print("F1 Score: ", f1)
print("Recall: ", recall)

"""**THEORETICAL PART**

**Question 1: Which model performed best and why do you think that is?**

**Answer 1:** Based on the result of the project, the accuracy of Neural Networks (NN) increased significantly - outperforming KNN. One possible explanation is that NN assigns weights to decisions and revisits them using the backpropagation algorithm to analyse errors and learn each time it is trained. KNN, on the other hand, works only once on all of the provided training data. Neural Networks can also extract data from longer context sequences.


**Question 2: Which features gave better results for each model? Explain.**

**Answer 2:** For KNN, embeddings gives better results because the context is being preserved. Embeddings greatly reduce the number of dimensions of high dimensional data like sparse categorical features. However, for random forest, Bag of words increases the accuracy because the model uses word frequency. The irrelevant words get filtered and only the relevant, most frequent ones are used for classification. 


**Question 3: What effect would increasing the classes to 150 have?**

**Answer 3:** Classifying tweets written within few candidates is simpler however, with respect to our project, we currently have 6 classes - i.e. Authors. Increasing the classes to 150 would decrease the accuracy of the model as seen while experimenting. Increasing authors from 5 to 6 decreased the accuracy from 88% to 72%. Based on the text pre-processing performed (i.e we removed everything except english words) and data collected per author is smaller compared to the number of authors. The number of similar words and styles would be seen more frequently which makes it harder to classify.
Our finding is backed by literature, where researchers found that increasing the number of authors caused a significant drop in accuracy. Koppel et al [1] found that performance decreases when the approach is applied to a large set which supports our findings. They have identified multiple features that can be used to characterize an author. This includes lexical features, stylistic features, and structural features. Based on our text pre-processing, we have removed most of the features which affect the accuracy.

References:
Koppel, M., Schler, J., and Argamon, S. (2011). Authorship attribution in the wild. Language Resources and Evaluation, 45(1):83–94.
 
 
**Question 4: Suggest improvements to text preparation, feature extraction, and models that can be made to perform this task better.**

**Answer 4:** Four main categories of stylometry based techniques can be used to generate a better representation of the feature vector for each tweet. The categories include Lexical and Character Features (based on the frequency of words i.e., bag of words), Syntactic Features (analyses of the syntax of text), Semantic Features (focuses on grammatical units of the text) and Application-Specific Features. When cleaning the data, the punctuations, frequency of verbs, adjectives, pronouns and adverbs should be kept for tagging. These stylometric features can be used for training the model.  
In addition to this, we used snscraper, to scrape the tweets. The scraper returns information like likes, retweets, hashtags, time and date of the posted tweet, quoted tweets, the links and urls embedded into the tweet in addition to the text part. This information can be helpful while doing  authorship attribution.  
As for the improvements to the models, LSTM (Long-Short Term Memory), a type of recureetn neural network can be used. LSTM layers learns the pattern, maintains the history and recognizes patterns in the data to make better predictions. LSTM keeps the required information after every iteration and discards the information which is not useable for further predictions.  

References:
“A Complete Guide to LSTM Architecture and Its Use in Text Classification.” Analytics India Magazine, 28 Feb. 2022, analyticsindiamag.com/a-complete-guide-to-lstm-architecture-and-its-use-in-text-classification.

Verma, Yugesh. “A Complete Guide to LSTM Architecture and Its Use in Text Classification.” Analytics India Magazine, 28 Feb. 2022, analyticsindiamag.com/a-complete-guide-to-lstm-architecture-and-its-use-in-text-classification.


**Question 5: What - in your understanding - are the applications of authorship attribution?**

**Answer 5:** The authorship attribution is the task of inferring characteristics of the author from a given document produced by that respective author, in this case, associating a tweet to the username. Other possible applications include:
Author Profiling:
The method of attributing an author based on the lexical, structure, and content of the document pinpoints the authors associated with the document. This can also aid in the identification of authors related to historical documents that remain disputed or unclear, helping out in archaeological departments and expansion in literature.
Plagiarism Detection:
The produced text documents are compared to a set of original online and offline documents to detect potentially ambiguous texts based on differences in writing style. It is possible to eliminate plagiarism by verifying the original owner of the piece of document which can aid in educational institutions and the academic realm.
Investigation in Cybercrime:
In today’s digital era, the boundary between ethical and unethical behaviour has blurred. By implementing authorship attribution we can evaluate the linguistic aspects of digital messages and compare them to the known writing styles.This can aid in the analysis of web spam, virus code, identity theft, censoring of inappropriate messages and digital terrorist activity.
"""